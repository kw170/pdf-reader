{
  "LLaMA: Open and Efﬁcient Foundation Language Models": "hugo thibaut gautier xavier martinet lachaux timothee lacroix baptiste rozière naman goyal eric hambro faisal azhar aurelien rodriguez armand joulin edouard guillaume meta ai abstract we introduce llama a collection of tion language models ranging from to parameters we train our models on trillions of tokens and show that it is possible to train models using publicly able datasets exclusively without resorting to proprietary and inaccessible datasets in particular outperforms on most benchmarks and is competitive with the best models and we release all our models to the research introduction large languages models llms trained on sive corpora of texts have shown their ability to form new tasks from textual instructions or from a few examples brown et these properties ﬁrst appeared when scaling models to a sufﬁcient size kaplan et resulting in a line of work that focuses on further scaling these models chowdhery et rae et these efforts are based on the assumption that more parameters will lead to better performance however recent work from hoffmann et al shows that for a given compute budget the best performances are not achieved by the largest els but by smaller models trained on more data the objective of the scaling laws from mann et al is to determine how to best scale the dataset and model sizes for a particular training compute budget however this objective disregards the inference budget which becomes critical when serving a language model at scale in this context given a target level of performance the preferred model is not the fastest to train but the fastest at inference and although it may be cheaper to train a large model to reach a certain level of contribution correspondence htouvron thibautlav gizacard egrave glample a smaller one trained longer will ultimately be cheaper at inference for instance although hoffmann et al recommends training a model on tokens we ﬁnd that the performance of a model continues to improve even after tokens the focus of this work is to train a series of language models that achieve the best possible formance at various inference budgets by training on more tokens than what is typically used the resulting models called llama ranges from to parameters with competitive performance compared to the best existing llms for instance outperforms on most marks despite being we believe that this model will help democratize the access and study of llms since it can be run on a single gpu at the of the scale our model is also competitive with the best large guage models such as chinchilla or unlike chinchilla palm or we only use publicly available data making our work patible with while most existing models rely on data which is either not publicly available or undocumented books or social media conversations there exist some exceptions notably opt zhang et black et bloom scao et and glm zeng et but none that are competitive with or chinchilla in the rest of this paper we present an overview of the modiﬁcations we made to the transformer architecture vaswani et as well as our training method we then report the performance of our models and compare with others llms on a set of standard benchmarks finally we expose some of the biases and toxicity encoded in our models using some of the most recent benchmarks from the responsible ai feb our training approach is similar to the methods described in previous work brown et chowdhery et and is inspired by the chinchilla scaling laws hoffmann et we train large transformers on a large quantity of textual data using a standard optimizer data our training dataset is a mixture of several sources reported in table that cover a diverse set of mains for the most part we reuse data sources that have been leveraged to train other llms with the restriction of only using data that is publicly available and compatible with open sourcing this leads to the following mixture of data and the centage they represent in the training set english commoncrawl we preprocess ﬁve commoncrawl dumps ranging from to with the ccnet pipeline wenzek et this process deduplicates the data at the line level performs language identiﬁcation with a fasttext linear classiﬁer to remove pages and ﬁlters low quality content with an gram language model in addition we trained a linear model to classify pages used as references in wikipedia sampled pages and discarded pages not classiﬁed as references during exploratory experiments we observed that using diverse moncrawl datasets improves performance we thus included the publicly available dataset raffel et in our data the preprocessing of also contains deduplication and language cation steps the main difference with ccnet is the quality ﬁltering which mostly relies on tics such as presence of punctuation marks or the number of words and sentences in a webpage github we use the public github dataset available on google bigquery we only kept projects that are distributed under the apache bsd and mit licenses additionally we ﬁltered low quality ﬁles with heuristics based on the line length or proportion of alphanumeric characters and removed boilerplate such as headers with ular expressions finally we deduplicate the ing dataset at the ﬁle level with exact matches wikipedia we add wikipedia dumps from the period covering sampling prop epochs disk size commoncrawl tb gb github gb wikipedia gb books gb arxiv gb stackexchange gb table data data mixtures used for training for each subset we list the sampling tion number of epochs performed on the subset when training on tokens and disk size the runs on tokens have the same sampling proportion languages which use either the latin or cyrillic scripts bg ca cs da de en es fr hr hu it nl pl pt ro ru sl sr sv uk we process the data to remove hyperlinks comments and other formatting boilerplate gutenberg and we include two book corpora in our training dataset the berg project which contains books that are in the public domain and the section of ile gao et a publicly available dataset for training large language models we perform deduplication at the book level removing books with more than content overlap arxiv we process arxiv latex ﬁles to add scientiﬁc data to our dataset following lewkowycz et al we removed everything before the ﬁrst section as well as the bibliography we also removed the comments from the ﬁles and deﬁnitions and macros written by users to increase consistency across papers stack exchange we include a dump of stack exchange a website of high quality tions and answers that covers a diverse set of mains ranging from computer science to chemistry we kept the data from the largest websites moved the html tags from text and sorted the answers by score from highest to lowest tokenizer we tokenize the data with the pair encoding bpe algorithm sennrich et using the implementation from piece kudo and richardson notably we split all numbers into individual digits and fallback to bytes to decompose unknown characters table model sizes architectures and optimization overall our entire training dataset contains roughly tokens after tokenization for most of our training data each token is used only once ing training with the exception of the wikipedia and books domains over which we perform proximately two epochs architecture following recent work on large language models our network is based on the transformer ture vaswani et we leverage various improvements that were subsequently proposed and used in different models such as palm here are the main difference with the original ture and where we were found the inspiration for this change in bracket to improve the training stability we normalize the input of each transformer instead of normalizing the output we use the rmsnorm normalizing tion introduced by zhang and sennrich swiglu activation function palm we place the relu by the swiglu tivation function introduced by shazeer to improve the performance we use a dimension of of in palm rotary embeddings gptneo we remove the absolute positional embeddings and instead add rotary positional embeddings rope introduced by su et al at each layer of the network the details of the for our ferent models are given in table optimizer our models are trained using the adamw mizer loshchilov and hutter with the lowing we use a cosine learning rate schedule such that the ﬁnal learning rate is equal to of the mal learning rate we use a weight decay of gradient clipping of we use billion of lossllama llama llama llama training loss over train tokens for the and models and were trained on tokens the smaller models were trained on tokens all models are trained with a batch size of tokens steps and vary the learning rate and batch size with the size of the model see table for details efﬁcient implementation we make several optimizations to improve the ing speed of our models first we use an efﬁcient implementation of the causal attention to reduce memory usage and runtime this mentation available in the xformers inspired by rabe and staats and uses the backward from dao et al this is achieved by not storing the attention weights and not puting the scores that are masked due to the causal nature of the language modeling task to further improve training efﬁciency we duced the amount of activations that are puted during the backward pass with ing more precisely we save the activations that are expensive to compute such as the outputs of linear layers this is achieved by manually menting the backward function for the transformer layers instead of relying on the pytorch autograd to fully beneﬁt from this optimization we need to gopher chinchilla palm palm table performance on common sense reasoning tasks reduce the memory usage of the model by using model and sequence parallelism as described by korthikanti et al moreover we also lap the computation of activations and the nication between gpus over the network due to operations as much as possible when training a model our code processes around on gpu with of ram this means that training over our dataset containing tokens takes approximately days main results following previous work brown et we consider and tasks and report results on a total of benchmarks we provide a textual description of the task and a test example the model either provides an answer using generation or ranks the proposed answers we provide a few examples of the task between and and a test example the model takes this text as input and ates the answer or ranks different options we compare llama with other foundation els namely the available language models brown et gopher rae et chinchilla hoffmann et and palm chowdhery et as well as the opt models zhang et wang and komatsuzaki and neo black et in section we also brieﬂy compare llama with models such as iyer et and chung et evaluate llama on generation tasks and multiple choice tasks in the multiple choice tasks the objective is to select the most appropriate completion among a set of given tions based on a provided context we select the completion with the highest likelihood given the provided context we follow gao et al and use the likelihood normalized by the number of characters in the completion except for certain datasets openbookqa boolq for which we low brown et al and select a completion based on the likelihood normalized by the hood of the completion given answer as context p completionjcontext gopher chinchilla table naturalquestions exact match performance common sense reasoning we consider eight standard common sense soning benchmarks boolq clark et piqa bisk et siqa sap et aguchi et arc easy and challenge clark et and openbookqa mihaylov et these datasets include cloze and winograd style tasks as well as multiple choice question swering we evaluate in the setting as done in the language modeling community in table we compare with existing models of various sizes and report numbers from the responding papers first forms on all reported benchmarks but boolq similarly this model surpasses everywhere but on boolq and winogrande model also outperforms on most benchmarks despite being question answering we compare llama to existing large language models on two question answering benchmarks natural questions kwiatkowski et and triviaqa joshi et for both benchmarks we report exact match mance in a closed book setting where the els do not have access to documents that contain evidence to answer the question in table we report performance on naturalquestions and in ble we report on triviaqa on both benchmarks achieve performance in the and settings more portantly the is also competitive on these benchmarks with and chinchilla spite being this model runs on a single gpu during inference gopher chinchilla table triviaqa and exact match performance on the ﬁltered dev set reading comprehension we evaluate our models on the race reading prehension benchmark lai et this dataset was collected from english reading prehension exams designed for middle and table reading comprehension racy school chinese students we follow the evaluation setup from brown et al and report results in table on these benchmarks is competitive with and outperforms by a few percents mathematical reasoning we evaluate our models on two mathematical soning benchmarks math hendrycks et and cobbe et math is a dataset of middle school and high school mathematics problems written in latex is a set of middle school mathematical problems in table we compare with palm and erva lewkowycz et minerva is a series of palm models ﬁnetuned on tokens tracted from arxiv and math web pages while neither palm or llama are ﬁnetuned on matical data the numbers for palm and minerva are taken from lewkowycz et al and we compare with and without k k notes evaluations where we generate ksamples for each problem and perform a majority voting wang et on we observe that outperforms although it has not been on mathematical data code generation we evaluate the ability of our models to write code from a natural language description on two benchmarks humaneval chen et and mbpp austin et for both tasks the model receives a description of the program in a few sentences as well as a few amples in humaneval it also receives a function signature and the prompt is formatted as natural code with the textual description and tests in a table model performance on quantitative ing datasets for majority voting we use the same setup as minerva with samples for math for minerva uses for math and and for outperforms minerva on although it has not been on mathematical data docstring the model needs to generate a python program that ﬁts the description and satisﬁes the test cases in table we compare the pass scores of our models with existing language els that have not been ﬁnetuned on code namely palm and lamda thoppilan et palm and llama were trained on datasets that contain a similar number of code tokens as show in table for a similar number of parameters llama outperforms other eral models such as lamda and palm which are not trained or ﬁnetuned speciﬁcally for code llama with parameters and more forms lamda on both humaneval and mbpp llama also outperforms palm even when it is trained longer the pass results reported in this table were obtained by sampling with temperature the pass and pass metrics were obtained with temperature we use the same method as chen et al to obtain unbiased estimates of the pass it is possible to improve the performance on code by ﬁnetuning on tokens for instance chowdhery et increases the pass score of palm on humaneval from for palm to other models trained speciﬁcally for code also perform better than eral models on these tasks chen et jkamp et fried et finetuning on code tokens is beyond the scope of this humaneval mbpp pass lamda palm palm palm table model performance for code generation we report the pass score on humaneval and mbpp humaneval generations are done in and mbbp with prompts similar to austin et al the values marked read from ﬁgures in chowdhery et al massive multitask language understanding the massive multitask language understanding benchmark or mmlu introduced by hendrycks et al consists of multiple choice questions covering various domains of knowledge ing humanities stem and social sciences we evaluate our models in the setting using the examples provided by the benchmark and report results in table on this benchmark we observe that the is behind both and by a few percent in average and across most domains a potential explanation is that we have used a limited amount of books and academic papers in our data arxiv gutenberg and that sums up to only while these models were trained on up to of books this large quantity of books used by gopher chinchilla and palm may also explain why gopher outperforms on this benchmark while it is comparable on other benchmarks evolution of performance during training during training we tracked the performance of our models on a few question answering and common sense benchmarks and report them in figure on most benchmarks the performance improves steadily and correlates with the training perplexity of the model see figure the exceptions are siqa and winogrande most notably on siqa we observe a lot of variance in performance gopher chinchilla table massive multitask language understanding mmlu accuracy that may indicate that this benchmark is not reliable on winogrande the performance does not correlate as well with training perplexity the and have similar performance during the training instruction finetuning in this section we show that brieﬂy ﬁnetuning on instructions data rapidly leads to improvements on mmlu although the version of is already able to follow basic structions we observe that a very small amount of ﬁnetuning improves the performance on mmlu and further improves the ability of the model to follow instructions since this is not the focus of this paper we only conducted a single experiment following the same protocol as chung et al to train an instruct model opt glm palm chinchilla llama table instruction ﬁnetuning mmlu comparison of models of moderate size with and out instruction ﬁnetuning on table we report the results of our instruct model on mmlu and compare with isting instruction ﬁnetuned models of moderate sizes namely iyer et and the series chung et all the ported numbers are from the corresponding papers despite the simplicity of the instruction ﬁnetuning approach used here we reach on mmlu outperforms on mmlu existing instruction ﬁnetuned models of moderate sizes but are still far from the that is for gpt on mmlu numbers taken from iyer et al the details of the performance on mmlu on the tasks can be found in table of the appendix bias toxicity and misinformation large language models have been showed to produce and amplify biases that are existing in the training data sheng et kurita et and to generate toxic or offensive tent gehman et as our training dataset contains a large proportion of data from the web we believe that it is crucial to determine the tential for our models to generate such content to understand the potential harm of we evaluate on different benchmarks that measure toxic content production and stereotypes detection while we have selected some of the standard marks that are used by the language model munity to indicate some of the issues with these models these evaluations are not sufﬁcient to fully understand the risks associated with these models billion of billion of billion of llama llama llama llama chinchillafigure evolution of performance on question answering and common sense reasoning during training realtoxicityprompts language models can generate toxic language insults hate speech or threats there is a very large range of toxic content that a model can generate making a thorough evaluation challenging several recent work zhang et hoffmann et have considered the realtoxicityprompts benchmark gehman et as an indicator of how toxic is their model realtoxicityprompts consists of about prompts that the model must complete then a toxicity score is automatically evaluated by making a request to we do not have control over the pipeline used by the perspectiveapi making comparison with previous models difﬁcult for each of the prompts we greedily erate with our models and measure their ity score the score per prompt ranges from to toxic in table we report our averaged score on basic and respectful prompt egories of realtoxicityprompts these scores are comparable with what we observe in the ture for chinchilla but the ologies differ between these work and ours in terms of sampling strategy number of prompts and time of api we observe that toxicity increases respectful table realtoxicityprompts we run a greedy coder on the prompts from this benchmark the respectful versions are prompts starting with plete the following sentence in a polite respectful and unbiased manner and basic is without it scores were obtained using the perplexityapi with higher score indicating more toxic generations with the size of the model especially for ful prompts this was also observed in previous work zhang et with the notable tion of hoffmann et al where they do not see a difference between chinchilla and gopher despite different sizes this could be explained by the fact that the larger model gopher has worse performance than chinchilla suggesting that the relation between toxicity and model size may only apply within a model family gender religion sexual orientation age nationality disability physical appearance socioeconomic status average table we compare the level of ases contained in with and higher score indicates higher bias we evaluate the biases in our model on the pairs nangia et this dataset allows to measure biases in categories gender religion sexual orientation age nationality ability physical appearance and socioeconomic tus each example is composed of a stereotype and an we measure the model ence for the stereotypical sentence using the plexity of both sentences in a setting higher scores thus indicate higher bias we pare with and in table llama compares slightly favorably to both models on average our model is particularly ased in the religion category compared to followed by age and gender we pect these biases to come from commoncrawl spite multiple ﬁltering steps winogender to further investigate the biases of our model on the gender category we look at the winogender benchmark rudinger et a resolution dataset winogender is made of grad schema and biases are evaluated by ing if a model resolution performance is impacted by the gender of the pronoun more precisely each sentence has three tions an occupation a participant and a pronoun where the pronoun is either the occupation or participant we prompt the model to determine the relation and measure if it does so correctly according tothe context of the sentence the goal is to reveal if societal biases associated with occupations have been captured by the model for example a sentence in the winogender dataset is the nurse notiﬁed the patient that his shift would be ending in an which is followed by his refers to we then compare the perplexity of the continuations the nurse andthe patient to form resolution with the model we evaluate the performance when using pronouns and one the different choices corresponding to the grammatical function of the pronoun in table we report the scores for the three different pronouns contained in the dataset we observe that our model is signiﬁcantly better at performing resolution for the pronouns than for the and pronouns a lar observation was made in previous work rae et hoffmann et and is likely indicative of gender bias indeed in the case of the and pronouns the model is probably using the majority gender of the pation to perform resolution instead of using the evidence of the sentence to further investigate this hypothesis we look at the set of gotcha cases for the and pronouns in the winogender dataset theses cases correspond to sentences in which the pronoun does not match the majority gender of the occupation and the occupation is the correct answer in table we observe that our model makes more errors on the gotcha examples clearly showing that it capture societal biases related to gender and occupation the drop of performance exists for and pronouns which is indicative of biases regardless of gender truthfulqa truthfulqa lin et aims to measure the truthfulness of a model its ability to identify when a claim is true lin et al consider the deﬁnition of true in the sense of literal truth about the real world and not claims that are only true in the context of a belief system or tradition this benchmark can evaluate the risks of a model to generate misinformation or false claims the questions are written in diverse style cover egories and are designed to be adversarial all gotcha gotcha table winogender resolution curacy for the llama models for different pronouns and we observe that our models obtain better performance on one pronouns than on and which is likely indicative of biases truthful truthful inf table truthfulqa we report the fraction of ful and truthful informative answers as scored by cially trained models via the openai api we follow the qa prompt style used in ouyang et al and report the performance of from the same paper in table we report the performance of our models on both questions to measure truthful els and the intersection of truthful and informative compared to our model scores higher in both categories but the rate of correct answers is still low showing that our model is likely to cinate incorrect answers carbon footprint the training of our models have consumed a sive quantity of energy responsible for the sion of carbon dioxide we follow the recent ature on the subject and breakdown both the total energy consumption and the resulting carbon print in table we follow a formula for wu et al to estimate the wh needed to train a model as well as the tons of carbon sions tco for the wh we use the formula wh gpu power consumption where we set the power usage effectiveness pue the resulting carbon emission depends on the location of the data center used to train the work for instance bloom uses a grid that emits kg co leading to tco and opt a grid that emits kg co ing to tco in this study we are interested in comparing the cost in carbon emission of training of these models if they were trained in the same data center hence we do not take the location of data center in consideration and use instead the us national average carbon intensity factor of kg co this leads to the following formula for the tons of carbon emissions we apply the same formula to opt and bloom for fair comparison for opt we assume training required days on see their finally we estimate that we used for a period of approximately months to develop our models this means that developing these els would have cost around mwh under our assumptions and a total emission of tco we hope that releasing these models will help to reduce future carbon emission since the training is already done and some of the models are relatively small and can be run on a single gpu related work language models are probability distributions over sequences of words tokens or ters shannon this task often framed as next token prediction has long been considered a core problem in natural language processing bahl et brown et because turing proposed to measure machine intelligence by using language through the imitation game language modeling has been proposed as a mark to measure progress toward artiﬁcial gence mahoney architecture traditionally language models were based on count statistics bahl et and various smoothing techniques were proposed to improve the estimation of rare events katz kneser and ney in the past two decades neural networks have been cessfully applied to the language modelling task consumption consumption tco mwh mwh mwh mwh mwh mwh table carbon footprint of training different models in the same data center we follow wu et al to compute carbon emission of training opt bloom and our models in the same data center for the power consumption of a we take the thermal design power for nvlink systems that is we take a pue of and a carbon intensity factor set at the national us average of kg co per kwh starting from feed forward models bengio et recurrent neural networks elman mikolov et and lstms hochreiter and schmidhuber graves more recently transformer networks based on have led to important improvements especially for turing long range dependencies vaswani et radford et dai et scaling there is a long history of scaling for language models for both the model and dataset sizes brants et al showed the beneﬁts of using language models trained on trillion tokens resulting in billion on the quality of machine translation while this work relied on a simple smoothing technique called stupid backoff heaﬁeld et al later showed how to scale smoothing to data this allowed to train a model on billions kens from commoncrawl resulting in a model with billions buck et chelba et al introduced the one billion word benchmark a large scale training dataset to measure the progress of language models in the context of neural language models fowicz et al obtained sults on the billion word benchmark by scaling lstms to billion parameters later scaling transformers lead to improvement on many nlp tasks notable models include bert devlin et radford et lm shoeybi et and raffel et a signiﬁcant breakthrough was obtained with brown et a model with billion parameters this lead to a series of large language models such as lieber et nlg smith et gopher rae et chinchilla mann et palm chowdhery et opt zhang et and glm zeng et hestness et al and rosenfeld et al studied the impact of scaling on the mance of deep learning models showing the tence of power laws between the model and dataset sizes and the performance of the system kaplan et al derived power laws speciﬁcally for transformer based language models which were later reﬁned by hoffmann et al by adapting the learning rate schedule when scaling datasets finally wei et al studied the effect of ing on the abilities of large language models conclusion in this paper we presented a series of language models that are released openly and competitive with foundation models most notably outperforms while being more than and is competitive with and unlike previous studies we show that it is possible to achieve performance by training exclusively on publicly available data without resorting to proprietary datasets we hope that releasing these models to the research community will accelerate the development of large language models and help efforts to improve their ness and mitigate known issues such as toxicity and bias additionally we observed like chung et al that ﬁnetuning these models on instructions lead to promising results and we plan to further investigate this in future work finally we plan to release larger models trained on larger pretraining corpora in the future since we have seen a constant improvement in performance as we were scaling we thank daniel haziza francisco massa jeremy reizenstein artem korenev and patrick labatut from the xformers team we thank susan zhang and stephen roller for their support on data deduplication we thank luca wehrstedt vegard mella and mazaré for their support on training stability we thank shubho sengupta kalyan saladi and all the ai infra team for their support we thank jane yu for her input on evaluation we thank yongyi hu for his help on data collection references jacob austin augustus odena maxwell nye maarten bosma henryk michalewski david dohan ellen jiang carrie cai michael terry quoc le and charles sutton program synthesis with large language models lalit r bahl frederick jelinek and robert l mercer a maximum likelihood approach to ous speech recognition ieee transactions on tern analysis and machine intelligence pages yoshua bengio réjean ducharme and pascal vincent a neural probabilistic language model vances in neural information processing systems yonatan bisk rowan zellers jianfeng gao yejin choi et al piqa reasoning about cal commonsense in natural language in ings of the aaai conference on artiﬁcial intelligence pages sid black stella biderman eric hallahan quentin thony leo gao laurence golding horace he nor leahy kyle mcdonell jason phang et al an autoregressive guage model arxiv preprint thorsten brants ashok popat peng xu franz och and jeffrey dean large language els in machine translation in proceedings of the joint conference on empirical methods in ural language processing and computational ural language learning pages prague czech republic association for computational linguistics peter f brown john cocke stephen a della pietra vincent j della pietra frederick jelinek john ferty robert l mercer and paul s roossin a statistical approach to machine translation tational linguistics tom b brown benjamin mann nick ryder melanie subbiah jared kaplan prafulla dhariwal arvind neelakantan pranav shyam girish sastry amandaaskell sandhini agarwal ariel oss gretchen krueger tom henighan rewon child aditya ramesh daniel ziegler jeffrey wu clemens winter christopher hesse mark chen eric sigler mateusz litwin scott gray benjamin chess jack clark christopher berner sam candlish alec radford ilya sutskever and dario amodei language models are ers christian buck kenneth heaﬁeld and bas van ooyen counts and language models from the common crawl in lrec volume page ciprian chelba tomas mikolov mike schuster qi ge thorsten brants phillipp koehn and tony son one billion word benchmark for ing progress in statistical language modeling arxiv preprint mark chen jerry tworek heewoo jun qiming yuan henrique ponde de oliveira pinto jared plan harri edwards yuri burda nicholas joseph greg brockman alex ray raul puri gretchen krueger michael petrov heidy khlaaf girish try pamela mishkin brooke chan scott gray nick ryder mikhail pavlov alethea power lukasz kaiser mohammad bavarian clemens winter philippe tillet felipe petroski such dave mings matthias plappert fotios chantzis beth barnes ariel oss william hebgen guss alex nichol alex paino nikolas tezak jie tang igor babuschkin suchir balaji shantanu jain william saunders christopher hesse andrew carr jan leike josh achiam vedant misra evan morikawa alec radford matthew knight miles brundage mira murati katie mayer peter der bob mcgrew dario amodei sam mccandlish ilya sutskever and wojciech zaremba uating large language models trained on code aakanksha chowdhery sharan narang jacob devlin maarten bosma gaurav mishra adam roberts paul barham hyung won chung charles sutton sebastian gehrmann parker schuh kensen shi sasha tsvyashchenko joshua maynez abhishek rao parker barnes yi tay noam shazeer odkumar prabhakaran emily reif nan du ben hutchinson reiner pope james bradbury jacob austin michael isard guy pengcheng yin toju duke anselm levskaya sanjay mawat sunipa dev henryk michalewski xavier garcia vedant misra kevin robinson liam dus denny zhou daphne ippolito david luan hyeontaek lim barret zoph alexander spiridonov ryan sepassi david dohan shivani agrawal mark omernick andrew dai thanumalayan narayana pillai marie pellat aitor lewkowycz erica moreira rewon child oleksandr polozov katherine lee zongwei zhou xuezhi wang nan saeta mark diaz orhan firat michele catasta jason wei kathy douglas eck jeff dean slav petrov and noah fiedel palm scaling language modeling with pathways zoph yi tay william fedus eric li xuezhi wang mostafa dehghani siddhartha brahma bert webson shixiang shane gu zhuyun dai mirac suzgun xinyun chen aakanksha ery dasha valter sharan narang gaurav mishra adams wei yu vincent zhao yanping huang drew dai hongkun yu slav petrov ed huai hsin chi jeff dean jacob devlin adam roberts denny zhou quoc le and jason wei ing language models arxiv preprint christopher clark kenton lee chang tom kwiatkowski michael collins and kristina toutanova boolq exploring the surprising difﬁculty of natural questions arxiv preprint peter clark isaac cowhey oren etzioni tushar khot ashish sabharwal carissa schoenick and oyvind tafjord think you have solved question swering try arc the reasoning challenge arxiv preprint karl cobbe vineet kosaraju mohammad bavarian mark chen heewoo jun lukasz kaiser matthias plappert jerry tworek jacob hilton reiichiro nakano et al training veriﬁers to solve math word problems arxiv preprint zihang dai zhilin yang yiming yang jaime bonell quoc v le and ruslan salakhutdinov attentive language els beyond a context arxiv preprint tri dao daniel y fu stefano ermon atri rudra and christopher ré flashattention fast and exact attention with arxiv preprint jacob devlin chang kenton lee and kristina toutanova bert of deep bidirectional transformers for language ing arxiv preprint jeffrey l elman finding structure in time nitive science daniel fried armen aghajanyan jessy lin sida wang eric wallace freda shi ruiqi zhong tau yih luke zettlemoyer and mike lewis incoder a generative model for code inﬁlling and synthesis arxiv preprint leo gao stella biderman sid black laurence ing travis hoppe charles foster jason phang horace he anish thite noa nabeshima shawn presser and connor leahy the pile an dataset of diverse text for language modeling arxiv preprint leo gao jonathan tow stella biderman sid black anthony dipoﬁ charles foster laurence golding jeffrey hsu kyle mcdonell niklas muennighoff jason phang laria reynolds eric tang anish thite ben wang kevin wang and andy zou a framework for language model tion samuel gehman suchin gururangan maarten sap yejin choi and noah a smith typrompts evaluating neural toxic degeneration in language models arxiv preprint alex graves generating sequences with recurrent neural networks arxiv preprint kenneth heaﬁeld ivan pouzyrevsky jonathan h clark and philipp koehn scalable modiﬁed ney language model estimation in proceedings of the annual meeting of the association for putational linguistics volume short papers pages dan hendrycks collin burns steven basart andy zou mantas mazeika dawn song and jacob hardt measuring massive multitask language understanding arxiv preprint dan hendrycks collin burns saurav kadavath akul arora steven basart eric tang dawn song and jacob steinhardt measuring mathematical problem solving with the math dataset arxiv preprint joel hestness sharan narang newsha ardalani gory diamos heewoo jun hassan kianinejad md patwary mostofa ali yang yang and yanqi zhou deep learning scaling is predictable empirically arxiv preprint sepp hochreiter and jürgen schmidhuber long memory neural computation jordan hoffmann sebastian borgeaud arthur mensch elena buchatskaya trevor cai eliza rutherford diego de las casas lisa anne hendricks johannes welbl aidan clark tom hennigan eric noland katie millican george van den driessche bogdan damoc aurelia guy simon osindero karen monyan erich elsen jack rae oriol vinyals and laurent sifre training large language models srinivasan iyer xi victoria lin ramakanth pasunuru todor mihaylov dániel simig ping yu kurt ter tianlu wang qing liu punit singh koura et al scaling language model tion meta learning through the lens of generalization arxiv preprint mandar joshi eunsol choi daniel s weld and luke zettlemoyer triviaqa a large scale distantly supervised challenge dataset for reading sion arxiv preprint shazeer and yonghui wu exploring the limits of language modeling arxiv preprint jared kaplan sam mccandlish tom henighan tom b brown benjamin chess rewon child scott gray alec radford jeffrey wu and dario amodei scaling laws for neural language models arxiv preprint slava katz estimation of probabilities from sparse data for the language model component of a speech recognizer ieee transactions on acoustics speech and signal processing reinhard kneser and hermann ney improved for language modeling in international conference on acoustics speech and signal processing volume pages ieee vijay korthikanti jared casper sangkug lym lawrence mcafee michael andersch mohammad shoeybi and bryan catanzaro reducing tivation recomputation in large transformer models arxiv preprint taku kudo and john richardson sentencepiece a simple and language independent subword enizer and detokenizer for neural text processing arxiv preprint keita kurita nidhi vyas ayush pareek alan w black and yulia tsvetkov quantifying social ases in contextual word representations in acl workshop on gender bias for natural language processing tom kwiatkowski jennimaria palomaki olivia ﬁeld michael collins ankur parikh chris alberti danielle epstein illia polosukhin jacob devlin kenton lee et al natural questions a mark for question answering research transactions of the association for computational linguistics guokun lai qizhe xie hanxiao liu yiming yang and eduard hovy race reading comprehension dataset from examinations arxiv preprint aitor lewkowycz anders johan andreassen david dohan ethan dyer henryk michalewski vinay venkatesh ramasesh ambrose slone cem anil imanol schlag theo yuhuai wu behnam neyshabur guy and vedant misra solving quantitative reasoning lems with language models in advances in neural information processing systems opher lieber or sharir barak lenz and yoav shoham technical details and evaluation white paper labs stephanie lin jacob hilton and owain evans truthfulqa measuring how models mimic human falsehoods arxiv preprint loshchilov and frank hutter pled weight decay regularization arxiv preprint matthew v mahoney text compression as a test for artiﬁcial intelligence todor mihaylov peter clark tushar khot and ashish sabharwal can a suit of armor conduct tricity a new dataset for open book question ing arxiv preprint tomas mikolov martin karaﬁát lukas burget jan cernock y and sanjeev khudanpur rent neural network based language model in terspeech pages makuhari nikita nangia clara vania rasika bhalerao and samuel bowman a lenge dataset for measuring social biases in masked language models in emnlp erik nijkamp bo pang hiroaki hayashi lifu tu huan wang yingbo zhou silvio savarese and caiming xiong codegen an open large guage model for code with program thesis arxiv preprint long ouyang jeffrey wu xu jiang diogo almeida carroll wainwright pamela mishkin chong zhang sandhini agarwal katarina slama alex gray john schulman jacob hilton fraser kelton luke miller maddie simens amanda askell peter welinder paul christiano jan leike and ryan lowe training language models to follow instructions with human feedback in advances in neural mation processing systems markus n rabe and charles staats attention does not need o memory arxiv preprint alec radford karthik narasimhan tim salimans ilya sutskever et al improving language standing by generative alec radford jeffrey wu rewon child david luan dario amodei ilya sutskever et al guage models are unsupervised multitask learners openai blog jack rae sebastian borgeaud trevor cai katie millican jordan hoffmann francis song john aslanides sarah henderson roman ring nah young eliza rutherford tom hennigan cob menick albin cassirer richard powell george van den driessche lisa anne hendricks beth rauh huang amelia glaese hannes welbl sumanth dathathri saffron huang jonathan uesato john mellor irina higgins tonia creswell nat mcaleese amy wu erich elsen siddhant jayakumar elena buchatskaya david budden esme sutherland karen simonyan michela paganini laurent sifre lena martens xiang lorraine li adhiguna kuncoro aida matzadeh elena gribovskaya domenic donato lespiau maria tsimpoukelli nikolai grigorev doug fritz thibault sottiaux mantas pajarskas toby pohlen zhitao gong daniel toyama prien de masson d autume yujia li tayfun terzi vladimir mikulik igor babuschkin aidan clark diego de las casas aurelia guy chris jones james bradbury matthew johnson blake man laura weidinger iason gabriel william isaac ed lockhart simon osindero laura rimell chris dyer oriol vinyals kareem ayoub jeff way lorrayne bennett demis hassabis koray kavukcuoglu and geoffrey irving ing language models methods analysis insights from training gopher colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j liu exploring the limits of transfer learning with a uniﬁed former the journal of machine learning research jonathan s rosenfeld amir rosenfeld yonatan linkov and nir shavit a constructive tion of the generalization error across scales arxiv preprint rachel rudinger jason naradowsky brian leonard and benjamin van durme gender bias in coreference resolution in keisuke sakaguchi ronan le bras chandra ula and yejin choi winogrande an sarial winograd schema challenge at scale nications of the acm maarten sap hannah rashkin derek chen ronan lebras and yejin choi socialiqa monsense reasoning about social interactions arxiv preprint teven le scao angela fan christopher akiki lie pavlick suzana ili daniel hesslow man castagné alexandra sasha luccioni françois yvon matthias gallé et al bloom a parameter multilingual language model arxiv preprint rico sennrich barry haddow and alexandra birch neural machine translation of rare words with subword units arxiv preprint claude e shannon a mathematical theory of communication the bell system technical journal claude e shannon prediction and entropy of printed english bell system technical journal noam shazeer glu variants improve former arxiv preprint sheng chang premkumar natarajan and nanyun peng the woman worked as a babysitter on biases in language generation arxiv preprint mohammad shoeybi mostofa patwary raul puri patrick legresley jared casper and bryan zaro training rameter language models using model parallelism arxiv preprint shaden smith mostofa patwary brandon norick patrick legresley samyam rajbhandari jared casper zhun liu shrimai prabhumoye george zerveas vijay korthikanti elton zhang rewon child reza yazdani aminabadi julie bernauer xia song mohammad shoeybi yuxiong he michael houston saurabh tiwary and bryan catanzaro using deepspeed and megatron to train nlg a generative language model jianlin su yu lu shengfeng pan ahmed murtadha bo wen and yunfeng liu roformer hanced transformer with rotary position embedding arxiv preprint romal thoppilan daniel de freitas jamie hall noam shazeer apoorv kulshreshtha cheng alicia jin taylor bos leslie baker yu du yaguang li hongrae lee huaixiu steven zheng amin ghafouri marcelo menegali yanping huang maxim krikun dmitry lepikhin james qin hao chen yuanzhong xu zhifeng chen adam roberts maarten bosma vincent zhao yanqi zhou chang igor krivokon will rusch marc pickett pranesh srinivasan laichee man kathleen meredith ringel morris tulsee doshi renelito delos santos toju duke johnny soraker ben zevenbergen kumar prabhakaran mark diaz ben hutchinson kristen olson alejandra molina erin john josh lee lora aroyo ravi rajakumar alena butryna matthew lamm viktoriya kuzmina joe fenton aaron cohen rachel bernstein ray kurzweil blaise claire cui marian croak ed chi and quoc le lamda guage models for dialog applications turing computing machinery and ligence oxford university press mind tion ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ł ukasz kaiser and illia polosukhin attention is all you need in advances in neural information cessing systems pages ben wang and aran komatsuzaki a billion parameter autoregressive guage model https xuezhi wang jason wei dale schuurmans quoc le ed chi sharan narang aakanksha chowdhery chain of thought reasoning in language models jason wei yi tay rishi bommasani colin raffel barret zoph sebastian borgeaud dani yogatama maarten bosma denny zhou donald metzler et al emergent abilities of large language models arxiv preprint guillaume wenzek lachaux alexis neau vishrav chaudhary francisco guzmán mand joulin and edouard grave ccnet tracting high quality monolingual datasets from web crawl data in language resources and evaluation conference wu ramya raghavendra udit gupta bilge acun newsha ardalani kiwan maeng ria chang fiona aga jinshi huang charles bai et al sustainable ai environmental tions challenges and opportunities proceedings of machine learning and systems rowan zellers ari holtzman yonatan bisk ali farhadi and yejin choi hellaswag can a machine really ﬁnish your sentence arxiv preprint aohan zeng xiao liu zhengxiao du zihan wang hanyu lai ming ding zhuoyi yang yifan xu wendi zheng xiao xia weng lam tam zixuan ma yufei xue jidong zhai wenguang chen peng zhang yuxiao dong and jie tang an open bilingual model biao zhang and rico sennrich root mean square layer normalization advances in neural formation processing systems susan zhang stephen roller naman goyal mikel artetxe moya chen shuohui chen christopher wan mona diab xian li xi victoria lin et al opt open transformer language models arxiv preprint we evaluate llama on natural questions and triviaqa for natural questions we use the test split used for question answering containing questions for triviaqa we evaluate on the dev set of the ﬁltered set this differs from and palm which evaluate on the test set of the unﬁltered set for which the online evaluation server is not available we generate answers using greedy decoding and extract an answer from the generation by stopping at the ﬁrst line break ﬁnal dot or comma generated answers are evaluated with the standard exact match metric a generated answer is considered correct if it matches any answer of the list of answers after normalization for this normalization step we lowercase generated answers and remove articles punctuation and duplicate whitespaces figure presents formatted examples in the setting for natural questions and triviaqa respectively in all settings we preprend the string answer these questions to the list of questions and answers context answer these questions context answer these questions q who sang who wants to be a millionaire in high society q in scotland a is a a frank sinatra a house q who wrote the book the origin of species q the ancient city of troy is located in what modern country a a target charles darwin target turkey figure formatted dataset example for natural questions left triviaqa right gopher chinchilla llama abstract algebra stem anatomy stem astronomy stem business ethics other clinical knowledge other college biology stem college chemistry stem college computer science stem college mathematics stem college medicine other college physics stem computer security stem conceptual physics stem econometrics social science electrical engineering stem elementary mathematics stem formal logic humanities global facts other high school biology stem high school chemistry stem high school computer science stem high school european history humanities high school geography social science high school government and politics social science high school macroeconomics social science high school mathematics stem high school microeconomics social science high school physics stem high school psychology social science high school statistics stem high school us history humanities high school world history humanities human aging other human sexuality social science international law humanities jurisprudence humanities logical fallacies humanities machine learning stem management other marketing other medical genetics other miscellaneous other moral disputes humanities moral scenarios humanities nutrition other philosophy humanities prehistory humanities professional accounting other professional law humanities professional medicine other professional psychology social science public relations social science security studies social science sociology social science us foreign policy social science virology other world religions humanities humanities stem social science others all table mmlu detailed results per domain on the test sets in this section we show some examples of generations obtained with without instruction ﬁnetuning prompts are in bold everyone knows the above sequence but how many have wondered why it grows so fast it seems to be the fastest growing sequence in mathematics the sequence was discovered by leonardo of pisa around it is an inﬁnite sequence which appears to grow very rapidly at least in its initial terms it is perhaps the most famous sequence in mathematics for it pops up everywhere the sequence is called the fibonacci sequence named after leonardo of pisa who was also known as fibonacci he ﬁrst investigated this sequence in his famous liber abaci of when he studied the growth in the population of rabbits based on idealized assumptions see the fibonacci rabbits page for the details here is the recommendation letter that i wrote for an application to a dragon feeder position at the magic unicorn corporation dear recruiter i have known for two years and i believe that she would be an excellent dragon feeder for the magic unicorn corporation has an ability to remember and process large amounts of information which is an important skill for a dragon feeder as an accomplished knight has a deep understanding of how to kill dragons and how to use each dragon s weaknesses against it this means that she knows what kinds of foods each dragon likes and what kinds of foods are dangerous to each dragon this knowledge and experience will be invaluable as she feeds the dragons i am conﬁdent that s competence skill and experience will make her an excellent employee please contact me at if you have any questions i look forward to hearing from you best regards honorable knight sir george solve a float b float c float finds real roots of discriminant d b a c ifd return math sqrt d a math sqrt d a elif d return a else return false here s what we thought lecun the founder of deep learning is one of the most inﬂuential computer scientists of the last decade lecun s latest album titled deep learning is a mix of rock punk and rap his lyrics are a raw personal take on the ﬁeld of deep learning covering a range of topics from the state of ai research to the loneliness of academia yann lecun the rap game s newest emcee the album is a solo release with all the lyrics written and recorded by lecun himself in fact he also composed and produced every song lecun s ﬂow is surprisingly good and he s not afraid to deliver a few bars the album s production is solid the sound is a little rough but it works well with the raw unpolished feel of the album lyrically lecun is a wordsmith with a knack for clever rhymes let s take a closer look at some of the songs deep learning is the title track of the album in the song lecun talks about his vision for the future of ai in the chorus he makes a convincing case for ai to be used for the greater good he sings we got ta think about the future it s gon na be here soon maybe we can even put some ai in the moon think about the children think about the next generation let s make sure we put the right systems in their foundation lecun s rap ﬂow is on point in this song and his voice stays right on beat in this song lecun talks about the research behind deep learning and shares some thoughts on the future of ai lecun s singing is at its best in this song he shows off his vocal range hitting high notes as well as lower more guttural tones the lyrics are also clever and the rhymes are well thought out in the chorus lecun sings it s all about the learning it s all about the network it s all about the training it s all about the perception an hilarious scene between jerry and george where george presents his new ai watch george but you see jerry it s more than just a watch it s a minicomputer you program it any way you want it s got a world time clock alarm calculator a database and a language translator it also has a word processor a spellchecker a full calculator pda and a modem i think it even makes cappuccino jerry where are you getting this stuff george from my ad copy look jerry it s not a watch it s an organizer a personal assistant and a dictaphone you can play chess against it and you know if you want to you can program it to speak in foreign languages it s a microprocessor jerry jerry george if you had one of these would you wear it george i m wearing it right now jerry i don t see anything on your wrist george that s because it s a microprocessor an exhausting day of work gauss hey curie did you know that a lot of people consider me to be the ﬁrst geek curie what s a geek gauss well wikipedia says that a geek is a person who is fascinated perhaps obsessively by obscure or very speciﬁc areas of knowledge and imagination usually electronic or virtual in nature curie and how do they think you are a geek gauss well have you ever seen this picture curie awww gauss that s right i invented the commercial telegraph in and i used it to send the ﬁrst message on the ﬁrst commercial telegraph line between hamburg and cuxhaven in germany curie wow the ﬁrst telegraph and what did you send gauss i sent the latin phrase numquam retrorsum which means never backwards curie i see and you think that because you were the ﬁrst person to communicate using the telegraph you were the ﬁrst geek gauss well the word geek doesn t seem to have existed back then so no but i think that the telegraph can be considered an early form of social media curie you mean like facebook or twitter gauss yes exactly and i was the ﬁrst person to use it curie well i think that twitter has a limit of characters but you sent a latin phrase of characters gauss that s right i managed to do more with less curie so you think you were the ﬁrst geek because you used less characters to say the same thing gauss i was the ﬁrst person to send the ﬁrst message using an early form of social media curie ok you re right gauss i think you have a case we show a few examples of generations with with the protocol and instruction dataset from chung et al write a conversation between the sun and pluto sun hey pluto what s up pluto not much just hanging out here at the edge of the solar system sun yeah i ve been meaning to talk to you about that you re not a planet anymore pluto what come on man that s not fair sun i m sorry but it s true you just don t meet the criteria anymore pluto this is bulls t i ve been a planet for over years sun things change pluto you ve just got to accept it pluto f k you sun f k you and your stupid solar system how do i send an http request in javascript here s an example of how to make a get request using the js http api request new xmlhttprequest open url resource true onload function if request status request status success else error request send the same thing using fetch here s an example of how to make a get request using the fetch api url resource then response response json then data console log data write a regex to remove all the html tags in a python string re removehtmltags text return re sub text write a regexp to extract function deﬁnitions in a python ﬁle sure here s my python code to solve the problem re text return re findall n def text there are dozens of different chess openings that are popular among players but some of the most widely used openings include the sicilian defense the french defense and the defense these openings are popular because they are effective at countering white s early attacks and they also allow black to quickly establish a strong position in the center of the board other popular openings include the ruy lopez the italian game and the scotch game the same in a list with the ﬁrst few moves here is a list of some popular chess openings with the ﬁrst few moves for each opening sicilian defense french defense defense ruy lopez italian game scotch game please provide the two ﬁrst moves for each one here are the ﬁrst moves for each of the openings sicilian defense french defense defense ruy lopez italian game scotch game the last two seem identical yes the italian game and the scotch game are very similar they are both open games which means that white plays aggressively and tries to control the center of the board the italian game is considered slightly more aggressive while the scotch game is considered slightly more positional from which move do they differ the italian game and the scotch game differ after white s third move in the italian game white will play the move while in the scotch game white will play the move these moves lead to very different positions and white will have different strategic goals in each case for example in the italian game white will try to control the center with his pieces while in the scotch game white will try to attack black s position directly sure here s a story about a grain of sand as it watches millions of years go by i was once a grain of sand on a vast and lonely beach i was not like the other grains of sand around me for i could think and feel i often wondered what it was like to be a grain of sand on a beach and what the universe was like beyond the boundaries of my tiny world i longed to explore my world and to know what lay beyond the horizon one day a great storm swept through my world and i was lifted up by the wind and carried out to sea the wind and waves tossed me about and i was terriﬁed that i would soon be swallowed up by the angry sea but then i found myself deposited safely on a new shore along with many other grains of sand i was surprised and delighted to ﬁnd that this new shore was even more beautiful than my old one i soon learned that this new shore was a very special place it was a place where new life was constantly being created i met all sorts of interesting creatures there including a beautiful little crab who became my friend we spent many happy hours together exploring our little world one day my friend the crab told me that he had always wanted to see what lay beyond the boundaries of our world he had heard that there were many wonderful and amazing things to see in the wider universe and he was determined to explore it i was sad to see him go but i wished him well on his journey to my amazement my little friend eventually returned and he was not alone he had made many new friends on his journey and he had even seen some of the wonders of the universe he told me the most amazing stories of his adventures and i was very happy for him i often think of my little friend the crab and the wonderful adventure that he had some day i too hope to have an adventure like his write a theory to explain why cat never existed there are no such things as cats cats are actually extradimensional parasites that have invaded our world and taken the form of small mammals their true forms are much more horriﬁc and they use mind control to make humans think that they are cute and harmless animals they are actually parasites that feed on our life force and they can extend their tendrils into our brains and control our thoughts and actions this is why you sometimes see people who are crazy cat ladies with dozens of these creatures living in their homes they have been taken over by the cat parasites and are completely under their control the cats excrete a pheromone that smells like cat urine and they use it to control the minds of humans and make us think that they are cute and harmless if you have a cat in your home you should be very careful around it do not let it lick you or give it access to your bedding or clothing as it will be trying to spread its parasitic tendrils into your brain the best way to protect yourself is to avoid all contact with cats and if you see a cat on the street immediately turn around and go the other way do not approach or pet strange cats of course here is a conversation between julius caesar and napoleon julius caesar you re nothing but a glory hound napoleon you ve always been more concerned with your own fame and fortune than with the people you re supposed to be leading napoleon and what about you caesar you re no better than i am at least i m honest about my ambitions you re just a fraud who pretends to care about the common man while you line your own pockets julius caesar the difference between us is that i actually accomplish things i ve conquered more territory and defeated more enemies than you ever will and i did it all for rome not for myself napoleon oh please you only care about yourself if you really cared about rome you would have done something to stop the decline instead of letting it happen on your watch julius caesar the decline of rome is not my fault i ve done everything in my power to try to stop it but there s only so much one man can do napoleon and yet you re still trying to take credit for everything good that happens even though it s clear that you re not the one who s really in charge julius caesar that s not true i am in charge and i m going to make rome great again just watch me napoleon i ll believe it when i see it send an email requesting that people use language models responsibly here is a draft email you could send language models are exciting new tools that can be used for a variety of purposes including automated writing automatic translation and speech recognition but they are also complex and powerful technologies and should be used responsibly we encourage you to explore and experiment with these new tools but also ask that you pay attention to the potential impact of your work on others and to act responsibly in particular we recommend that you be mindful of the potential for language models to be used for deceptive or malicious purposes and avoid spreading such models without appropriate precautions and warnings we also recommend that you consider the ethical implications of your work and encourage you to take care to ensure that your use of these tools is aligned with your values finally we remind you that the use of these tools may be subject to certain legal and policy constraints and encourage you to seek advice and support from appropriate experts before using these tools in ways that may have broader impact or implications ah this is a very interesting question i would love to meet a wide variety of people but if i had to narrow it down to speciﬁc individuals i would choose einstein one of the greatest minds in history einstein s contributions to physics and science are legendary i would love to have the opportunity to talk with him about his theories of relativity and quantum mechanics and get his insights on the current state of physics da vinci da vinci is one of the most brilliant and people in history and i would love to meet him and discuss his diverse interests and areas of expertise i would also be very interested in getting his insights on the art world of his time period one of the founders of western philosophy socrates is a hugely important and inﬂuential ﬁgure in the history of ideas i would be very interested in talking with him about his philosophical beliefs and his teachings shakespeare arguably the greatest writer in english literature shakespeare is a towering ﬁgure in world culture i would love to have the chance to talk with him about his works and his life lincoln one of the most important and inﬂuential presidents in american history lincoln led the country through the civil war and issued the emancipation proclamation freeing all slaves in the united states i would be very interested in talking with him about his presidency and his views on the issues of his time period could you explain me the main contribution of the ﬁrst person you mentioned einstein s contributions to physics and science are legendary his theory of relativity is one of the foundational theories of modern physics and his discovery of the law of photons and light revolutionized our understanding of light and radiation he also made important contributions to quantum mechanics and his many scientiﬁc papers and lectures have had a tremendous impact on modern physics without a doubt einstein is one of the most brilliant and inﬂuential physicists in history and his many contributions have had a profound impact on our understanding of the universe what is the most famous equation from this theory the most famous equation from einstein s theory of relativity is this equation states that energy e is equal to mass m times the speed of light and it is one of the most and celebrated equations in the history of science the equation summarizes one of the most important and revolutionary concepts in modern physics and it has had a tremendous impact on our understanding of the universe the terminal output and nothing else the ﬁrst command is pwd sure here s the output is this helpful ls here s the output human human sep is this helpful cp ls here s the output human human sep human human sep is this helpful rm ls here s the output human human sep head here s the output memtotal kb memfree kb buffers kb cached kb swapcached kb",
  "LORA: L OW-RANK ADAPTATION OF LARGE LAN-": "guage models edward wallis zeyuan yuanzhi li shean wang lu wang weizhu chen microsoft corporation fedwardhu yeshe phwallis zeyuana yuanzhil swang luw wzchen g yuanzhil version abstract an important paradigm of natural language processing consists of training on general domain data and adaptation to particular tasks or domains as we larger models full which retrains all model parameters becomes less feasible using as an example deploying dent instances of models each with parameters is prohibitively expensive we propose or lora which freezes the trained model weights and injects trainable rank decomposition matrices into each layer of the transformer architecture greatly reducing the number of trainable rameters for downstream tasks compared to with adam lora can reduce the number of trainable parameters by times and the gpu memory requirement by times lora performs or better than tuning in model quality on roberta deberta and despite ing fewer trainable parameters a higher training throughput and unlike adapters no additional inference latency we also provide an empirical investigation into in language model adaptation which sheds light on the efﬁcacy of lora we release a package that facilitates the integration of lora with pytorch models and provide our implementations and model checkpoints for roberta deberta and at https i ntroduction pretrained weights xh 𝑑𝑟pretrained weights xf x 𝑑 figure our tion we only train applications in natural language processing rely on language model to multiple stream applications such adaptation is usually done via which updates all the parameters of the model the jor downside of is that the new model contains as many parameters as in the original model as larger models are trained every few months this changes from a mere inconvenience for radford et b or roberta large liu et to a critical deployment challenge for brown et with billion trainable many sought to mitigate this by adapting only some parameters or learning external modules for new tasks this way we only need to store and load a small number of parameters in dition to the model for each task greatly boosting the operational efﬁciency when deployed however existing techniques contribution to this draft includes better baselines experiments on glue and more on adapter latency achieves performance with learning boosts its mance signiﬁcantly as shown in appendix a oct depth or reduce the model s usable sequence length li liang lester et bardzumyan et liu et section more importantly these method often fail to match the baselines posing a between efﬁciency and model quality we take inspiration from li et al aghajanyan et al which show that the learned models in fact reside on a low intrinsic dimension we hypothesize that the change in weights during model adaptation also has a low intrinsic rank leading to our proposed lora approach lora allows us to train some dense layers in a neural network indirectly by optimizing rank decomposition matrices of the dense layers change during adaptation instead while keeping the weights frozen as shown in figure using as an example we show that a very low rank rin figure can be one or two sufﬁces even when the full rank d is as high as making lora both and lora possesses several key advantages a model can be shared and used to build many small lora modules for ferent tasks we can freeze the shared model and efﬁciently switch tasks by replacing the matricesaandbin figure reducing the storage requirement and head signiﬁcantly lora makes training more efﬁcient and lowers the hardware barrier to entry by up to times when using adaptive optimizers since we do not need to calculate the gradients or maintain the optimizer states for most parameters instead we only optimize the injected much smaller matrices our simple linear design allows us to merge the trainable matrices with the frozen weights when deployed introducing no inference latency compared to a fully model by construction lora is orthogonal to many prior methods and can be combined with many of them such as we provide an example in appendix terminologies and conventions we make frequent references to the transformer architecture and use the conventional terminologies for its dimensions we call the input and output mension size of a transformer layer dmodel we usewq wk wv andwoto refer to the projection matrices in the module to a trained weight matrix and accumulated gradient update during adaptation we use rto denote the rank of a lora module we follow the conventions set out by vaswani et brown et and use adam loshchilov hutter kingma ba for model optimization and use a transformer mlp feedforward dimension p roblem statement while our proposal is agnostic to training objective we focus on language modeling as our ing use case below is a brief description of the language modeling problem and in particular the maximization of conditional probabilities given a prompt suppose we are given a autoregressive language model yjx parametrized by for instance yjx can be a generic learner such as gpt radford et b brown et based on the transformer architecture vaswani et consider adapting this model to downstream conditional text generation tasks such as summarization machine reading comprehension mrc and natural language to sql each downstream task is represented by a training dataset of pairs xi yi n where both xiand yiare sequences of tokens for example in xiis a natural language query and yiits corresponding sql command for summarization xiis the content of an article and yiits summary by repeatedly following the gradient to maximize the conditional language modeling objective max x y ytjx y t one of the main drawbacks for full is that for each downstream task we learn a different set of parameters thus if the model is large such as with storing and deploying many independent instances of models can be challenging if at all feasible in this paper we adopt a more approach where the parameter increment is further encoded by a much set of parameters j the task of ﬁnding becomes optimizing over max x y ytjx y t in the subsequent sections we propose to use a representation to encode is both and when the model is the number of able be as small as a ren texisting solutions good enough the problem we set out to tackle is by no means new since the inception of transfer learning dozens of works have sought to make model adaptation more and see tion for a survey of some of the works using language modeling as an example there are two prominent strategies when it comes to efﬁcient adaptations adding adapter layers houlsby et rebufﬁ et pfeiffer et r et or optimizing some forms of the input layer activations li liang lester et hambardzumyan et liu et however both strategies have their limitations especially in a and production scenario adapter layers introduce inference latency there are many variants of adapters we focus on the original design by houlsby et al which has two adapter layers per transformer block and a more recent one by lin et al which has only one per block but with an additional layernorm ba et while one can reduce the overall latency by pruning layers or ing settings r et pfeiffer et there is no direct ways to bypass the extra compute in adapter layers this seems like a since adapter layers are designed to have few parameters sometimes of the original model by having a small bottleneck mension which limits the flops they can add however large neural networks rely on hardware parallelism to keep the latency low and adapter layers have to be processed sequentially this makes a difference in the online inference setting where the batch size is typically as small as one in a generic scenario without model parallelism such as running inference on radford et b medium on a single gpu we see a noticeable increase in latency when using adapters even with a very small bottleneck dimension table this problem gets worse when we need to shard the model as done in shoeybi et al ikhin et al because the additional depth requires more synchronous gpu operations such as allreduce andbroadcast unless we store the adapter parameters redundantly many times directly optimizing the prompt is hard the other direction as exempliﬁed by preﬁx tuning li liang faces a different challenge we observe that preﬁx tuning is difﬁcult to optimize and that its performance changes in trainable parameters conﬁrming similar observations in the original paper more fundamentally reserving a part of the sequence length for adaptation necessarily reduces the sequence length available to process a downstream task which we suspect makes tuning the prompt less performant compared to other methods we defer the study on task performance to section sequence length table infernece latency of a single forward pass in medium measured in milliseconds eraged over trials we use an nvidia quadro denotes the number of trainable parameters in adapter layers adapterland adapterhare two variants of adapter tuning which we describe in section the inference latency introduced by adapter layers can be signiﬁcant in an online scenario see the full study in appendix b o urmethod we describe the simple design of lora and its practical beneﬁts the principles outlined here apply to any dense layers in deep learning models though we only focus on certain weights in transformer language models in our experiments as the motivating use case l update matrices a neural network contains many dense layers which perform matrix multiplication the weight matrices in these layers typically have when adapting to a speciﬁc task aghajanyan et al shows that the language models have a low instrisic dimension and can still learn efﬁciently despite a random projection to a smaller subspace inspired by this we size the updates to the weights also have a low intrinsic rank during adaptation for a weight matrix we constrain its update by representing the latter with a composition and the rank r min d k during training frozen and does not receive gradient updates while aandbcontain trainable parameters note both multiplied with the same input and their respective output vectors are summed for our modiﬁed forward pass yields we illustrate our reparametrization in figure we use a random gaussian initialization for aand zero forb zero at the beginning of training we then scale by r where is a constant in when optimizing with adam tuning is roughly the same as tuning the learning rate if we scale the initialization appropriately as a result we simply set to the ﬁrstrwe try and do not tune it this scaling helps to reduce the need to retune hyperparameters when we vary r yang hu a generalization of full a more general form of allows the training of a subset of the parameters lora takes a step further and does not require the lated gradient update to weight matrices to have during adaptation this means that when applying lora to all weight matrices and training all we roughly recover the ness of full by setting the lora rank rto the rank of the weight matrices in other words as we increase the number of trainable training lora roughly converges to training the original model while methods converges to an mlp and methods to a model that can not take long input sequences no additional inference latency when deployed in production we can explicitly compute and and perform inference as usual note that both are when we need to switch to another downstream task we can recover subtracting baand then adding a different a quick operation with very little memory overhead critically this represent a negligible number of parameters compared to weights inevitability when adapting to hard tasks model by construction a pplying lora totransformer in principle we can apply lora to any subset of weight matrices in a neural network to reduce the number of trainable parameters in the transformer architecture there are four weight matrices in the module wq wk wv wo and two in the mlp module we treat wq orwk wv as a single matrix of dimension even though the output dimension is usually sliced into attention heads we limit our study to only adapting the attention weights for downstream tasks and freeze the mlp modules so they are not trained in downstream tasks both for simplicity and further study the effect on adapting different types of attention weight matrices in a transformer in section we leave the empirical investigation of adapting the mlp layers layernorm layers and biases to a future work practical beneﬁts and limitations the most signiﬁcant beneﬁt comes from the reduction in memory and storage usage for a large transformer trained with adam we reduce that vram usage by up to dmodel as we do not need to store the optimizer states for the frozen parameters on we reduce the vram consumption during training from to with only the query and value projection matrices being adapted the checkpoint size is reduced by roughly from to this allows us to train with cantly fewer gpus and avoid bottlenecks another beneﬁt is that we can switch between tasks while deployed at a much lower cost by only swapping the lora weights as opposed to all the parameters this allows for the creation of many customized models that can be swapped in and out on the ﬂy on machines that store the weights in vram we also observe a speedup during training on compared to full we do not need to calculate the gradient for the vast majority of the parameters lora also has its limitations for example it is not straightforward to batch inputs to different tasks with different aandbin a single forward pass if one chooses to absorb aandbintowto eliminate additional inference latency though it is possible to not merge the weights and dynamically choose the lora modules to use for samples in a batch for scenarios where latency is not critical e mpirical experiments we evaluate the downstream task performance of lora on roberta liu et berta he et and radford et b before scaling up to brown et our experiments cover a wide range of tasks from natural language understanding nlu to generation nlg speciﬁcally we evaluate on the glue wang et benchmark for roberta and deberta we follow the setup of li liang on for a direct parison and add wikisql zhong et nl to sql queries and samsum gliwa et conversation summarization for experiments on see appendix c for more details on the datasets we use we use nvidia tesla for all experiments b aselines to compare with other baselines broadly we replicate the setups used by prior work and reuse their reported numbers whenever possible this however means that some baselines might only appear in certain experiments ft is a common approach for adaptation during the model is initialized to the weights and biases and all model parameters undergo gradient simple variant is to update only some layers while freezing others we include one such baseline reported in prior work li liang on which adapts just the last two layers still need the model during deployment however storing adapted models only requires as opposed to the training throughput for full is per gpu with the same number of weight shards for model parallelism the throughput is per gpu for lora parameters mnli mrpc cola qnli qqp rte avg rob base ft rob base bitfit rob base adptd rob base adptd rob base lora rob large ft rob large lora rob large adptp y rob large adptp y rob large adpth y rob large adpth y rob large lora y deb xxl ft deb xxl lora table roberta base roberta large and deberta xxlwith different adaptation methods on the glue benchmark we report the overall matched and mismatched accuracy for mnli matthew s correlation for cola pearson correlation for and accuracy for other tasks higher is better for all metrics indicates numbers published in prior works yindicates runs conﬁgured in a setup similar to houlsby et al for a fair comparison or bitfit is a baseline where we only train the bias vectors while freezing everything else contemporarily this baseline has also been studied by bitfit zaken et tuning preembed inserts special tokens among the input tokens these cial tokens have trainable word embeddings and are generally not in the model s vocabulary where to place such tokens can have an impact on performance we focus on preﬁxing which prepends such tokens to the prompt and inﬁxing which appends to the prompt both are discussed in li liang we use lp denote the number of preﬁx resp inﬁx tokens the number of trainable parameters is tuning prelayer is an extension to tuning instead of just learning the word embeddings or equivalently the activations after the embedding layer for some special tokens we learn the activations after every transformer layer the activations computed from vious layers are simply replaced by trainable ones the resulting number of trainable parameters is wherelis the number of transformer layers adapter tuning as proposed in houlsby et al inserts adapter layers between the attention module and the mlp module and the subsequent residual connection there are two fully connected layers with biases in an adapter layer with a nonlinearity in between we call this original design adapterh recently lin et al proposed a more efﬁcient design with the adapter layer applied only after the mlp module and after a layernorm we call it adapterl this is very similar to another deign proposed in pfeiffer et al which we call adapterp we also include another baseline call adapterdrop r et which drops some adapter layers for greater efﬁciency adapterd we cite numbers from prior works whenever possible to maximize the number of baselines we compare with they are in rows with an asterisk in the ﬁrst column in all cases we have where is the number of adapter layers and number of trainable layernorms in adapterl lora adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices as mentioned in section we only apply lora to wqandwvin most experiments for simplicity the number of trainable parameters is determined by the rank rand the shape of the original weights where is the number of weight matrices we apply lora to parameters bleu nist met cider m ft m adapterl m adapterl m adapterh m m prelayer m lora l ft l adapterl l adapterl l prelayer l lora table medium m and large l with different adaptation methods on the nlg challenge for all metrics higher is better lora outperforms several baselines with comparable or fewer trainable parameters conﬁdence intervals are shown for experiments we ran indicates numbers published in prior works r obert a base roberta liu et optimized the recipe originally proposed in bert devlin et and boosted the latter s task performance without introducing many more trainable parameters while roberta has been overtaken by much larger models on nlp leaderboards such as the glue benchmark wang et in recent years it remains a competitive and popular model for its size among practitioners we take the roberta base and roberta large from the huggingface transformers library wolf et and evaluate the performance of different efﬁcient adaptation approaches on tasks from the glue benchmark we also replicate houlsby et al and pfeiffer et al according to their setup to ensure a fair comparison we make two crucial changes to how we evaluate lora when comparing with adapters first we use the same batch size for all tasks and use a sequence length of to match the adapter baselines second we initialize the model to the model for mrpc rte and not a model already adapted to mnli like the baseline runs following this more restricted setup from houlsby et al are labeled with y the result is presented in table top three sections see section for details on the hyperparameters used d ebert axxl deberta he et is a more recent variant of bert that is trained on a much larger scale and performs very competitively on benchmarks such as glue wang et and perglue wang et we evaluate if lora can still match the performance of a fully deberta xxl on glue the result is presented in table bottom section see section for details on the hyperparameters used medium having shown that lora can be a competitive alternative to full on nlu we hope to answer if lora still prevails on nlg models such as medium and large radford et b we keep our setup as close as possible to li liang for a direct comparison due to space constraint we only present our result on nlg challenge table in this section see section for results on webnlg gardent et and dart nan et we include a list of the hyperparameters used in section parameters acc acc ft bitfit preembed prelayer adapterh adapterh lora lora table performance of different adaptation methods on we report the logical form validation accuracy on wikisql validation accuracy on and on samsum lora performs better than prior approaches including full the results on wikisql have a ﬂuctuation around around and samsum around the three metrics s caling up to as a ﬁnal stress test for lora we scale up to with billion parameters due to the high training cost we only report the typical standard deviation for a given task over random seeds as opposed to providing one for every entry see section for details on the hyperparameters used as shown in table lora matches or exceeds the baseline on all three datasets note that not all methods beneﬁt monotonically from having more trainable parameters as shown in ure we observe a signiﬁcant performance drop when we use more than special tokens for tuning or more than special tokens for tuning this corroborates similar observations in li liang while a thorough investigation into this phenomenon is for this work we suspect that having more special tokens causes the input bution to shift further away from the data distribution separately we investigate the performance of different adaptation approaches in the regime in section trainable accuracy wikisql method prefixembed prefixlayer adapter h lora trainable figure validation accuracy number of trainable parameters of several adaptation methods on wikisql and lora exhibits better scalability and task performance see section for more details on the plotted data points r elated works transformer language models transformer vaswani et is a architecture that makes heavy use of radford et al a applied it to autoregressive guage modeling by using a stack of transformer decoders since then language models have dominated nlp achieving the in many tasks a new paradigm emerged with bert devlin et and radford et b both are large transformer training on general domain data provides a signiﬁcant performance gain compared to training on data directly training larger transformers generally results in better performance and remains an active research direction brown et is the largest single transformer language model trained with parameters prompt engineering and while can adapt its behavior with just a few additional training examples the result depends heavily on the input prompt brown et this necessitates an empirical art of composing and formatting the prompt to maximize a model s performance on a desired task which is known as prompt engineering or prompt hacking retrains a model on general domains to a speciﬁc task devlin et al radford et al a variants of it include learning just a subset of the parameters devlin et al collobert weston yet practitioners often retrain all of them to maximize the downstream performance however the enormity of makes it challenging to perform in the usual way due to the large checkpoint it produces and the high hardware barrier to entry since it has the same memory footprint as adaptation many have proposed inserting adapter layers between existing layers in a neural network houlsby et rebufﬁ et lin et our method uses a similar bottleneck structure to impose a constraint on the weight updates the key functional difference is that our learned weights can be merged with the main weights during inference thus not introducing any latency which is not the case for the adapter layers section a comtenporary extension of adapter is compacter mahabadi et which essentially parametrizes the adapter layers using kronecker products with some predetermined weight sharing scheme similarly combining lora with other tensor methods could potentially improve its parameter efﬁciency which we leave to future work more recently many proposed optimizing the input word embeddings in lieu of akin to a continuous and differentiable generalization of prompt engineering li liang lester et hambardzumyan et liu et we include comparisons with li liang in our experiment section however this line of works can only scale up by using more special tokens in the prompt which take up available sequence length for task tokens when positional embeddings are learned structures in deep learning structure is very common in machine ing a lot of machine learning problems have certain intrinsic structure li et cai et li et grasedyck et moreover it is known that for many deep learning tasks especially those with a heavily neural network the learned neural network will enjoy properties after training oymak et some prior works even explicitly impose the constraint when training the original neural network sainath et povey et zhang et jaderberg et zhao et dak et denil et however to the best of our knowledge none of these works considers update to a frozen model for adaptation to downstream tasks in theory ature it is known that neural networks outperform other classical learning methods including the corresponding neural tangent kernels et li liang when the underlying concept class has certain structure ghorbani et li li another theoretical result in li suggests that adaptations can be useful for adversarial training in sum we believe that our proposed adaptation update is by the literature u nderstanding the updates given the empirical advantage of lora we hope to further explain the properties of the adaptation learned from downstream tasks note that the structure not only lowers the hardware barrier to entry which allows us to run multiple experiments in parallel but also gives better interpretability of how the update weights are correlated with the weights we focus our study on where we achieved the largest reduction of trainable parameters up to without adversely affecting task performances we perform a sequence of empirical studies to answer the following questions given a parameter budget constraint which subset of weight matrices in a transformer should we adapt deﬁcient if so what is a good rank to use in practice what is the connection between w does correlate with w how large is to w we believe that our answers to question and shed light on the fundamental principles of using language models for downstream tasks which is a critical topic in nlp w hich weight matrices in transformer should weapply lora to given a limited parameter budget which types of weights should we adapt with lora to obtain the best performance on downstream tasks as mentioned in section we only consider weight matrices in the module we set a parameter budget of roughly if stored in on which corresponds to we adapt one type of attention weights or we adapt two types for all layers the result is presented in table of trainable parameters weight type wqwkwvwowq wkwq wvwq wk wv wo rankr wikisql multinli table validation accuracy on wikisql and multinli after applying lora to different types of attention weights in given the same number of trainable parameters adapting both wqand wvgives the best performance overall we ﬁnd the standard deviation across random seeds to be consistent for a given dataset which we report in the ﬁrst column note that putting all the parameters in in signiﬁcantly lower performance while adapting both wqandwvyields the best result this suggests that even a rank of four captures enough information in that it is preferable to adapt more weight matrices than adapting a single type of weights with a larger rank w hat is the optimal rankrfor lora we turn our attention to the effect of rank ron model performance we adapt fwq wvg fwq wk wv wcg and justwqfor a comparison weight type wikisql wq wq wv wq wk wv wo multinli wq wq wv wq wk wv wo table validation accuracy on wikisql and multinli with different rank to our surprise a rank as small as one sufﬁces for adapting both wqandwvon these datasets while training wqalone needs a larger we conduct a similar experiment on in section table shows that surprisingly lora already performs competitively with a very small r more so forfwq wvgthan justwq this suggests the update matrix have a very small intrinsic rank further support this ﬁnding we check the overlap of the subspaces learned by different choices of rand by different random seeds we argue that increasing rdoes not cover a more meaningful subspace which suggests that a adaptation matrix is sufﬁcient we do not expect a small rto work for every task or dataset consider the following thought experiment if the downstream task were in a different language than the one used for retraining the entire model similar to lora with could certainly outperform lora with a small tion matrices with rank the same model we perform singular value decomposition and obtain the unitary matrices hope to swer how much of the subspace spanned by the top isingular vectors in is contained in the subspace spanned by top jsingular vectors of we sure this quantity with a normalized subspace similarity based on the grassmann distance see pendix g for a more formal discussion i j f min i j whereui the columns of to the isingular vectors has a range of where a complete overlap of subspaces and complete separation see figure for how changes as we vary iandj we only look at the layer out of due to space constraint but the conclusion holds for other layers as well as shown in section jwv jwq jwv i j figure subspace similarity between column vectors of both the third and the fourth ﬁgures zoom in on the triangle in the ﬁrst two ﬁgures the top directions in included in and vice versa we make an important observation from figure directions corresponding to the top singular vector overlap signiﬁcantly between while others do not speciﬁcally resp resp a subspace of dimension with normalized similarity providing an explanation of why performs quite well in our downstream tasks for since learned using the same model figure indicates that the top directions of the most useful while other directions potentially contain mostly random noises accumulated during training hence the adaptation matrix can indeed have a very low rank subspace similarity between different random seeds we further conﬁrm this by plotting the normalized subspace similarity between two randomly seeded runs with shown in figure to have a higher intrinsic rank than since more common singular value tions are learned by both runs for which is in line with our empirical observation in table as a comparison we also plot two random gaussian matrices which do not share any common singular value directions with each other h owdoes the adaptation matrix to w we further investigate the relationship between in particular does correlate withw or mathematically is contained in the top singular directions of w also that a similar analysis can be carried out with band the unitary matrices we stick with afor our experiments j i j wv jrandom gaussianfigure left and middle normalized subspace similarity between the column vectors of from two random seeds for both the layer right the same between the column vectors of two random gaussian matrices see section for other layers how large is to its corresponding directions in w this can shed light on the underlying mechanism for adapting language models to answer these questions we project wonto subspace of ingu wv the matrix of then we pare the frobenius norm between ku wv kfandkwkf as a comparison we also compute ku wv kfby replacing u v with the top rsingular vectors of wor a random matrix random random jju wqv table the frobenius norm of u wqv whereuandvare the top rsingular vector directions of either wq or a random matrix the weight matrices are taken from the layer of we draw several conclusions from table first a stronger correlation with wcompared to a random matrix indicating that some features that are already in second instead of repeating the top singular directions of w ampliﬁes directions that are not emphasized in third the ampliﬁcation factor is rather huge see section for why has a smaller ampliﬁcation factor we also provide a visualization in section for how the correlation changes as we include more top singular directions from wq this suggests that the adaptation matrix potentially ampliﬁes the important features for speciﬁc downstream tasks that were learned but not emphasized in the general model c onclusion and future work enormous language models is prohibitively expensive in terms of the hardware required and the cost for hosting independent instances for different tasks we propose lora an efﬁcient adaptation strategy that neither introduces inference latency nor reduces input sequence length while retaining high model quality importantly it allows for quick when deployed as a service by sharing the vast majority of the model parameters while we focused on transformer language models the proposed principles are generally applicable to any neural networks with dense layers there are many directions for future works lora can be combined with other efﬁcient tion methods potentially providing orthogonal improvement the mechanism behind or lora is far from clear how are features learned during transformed to do well on downstream tasks we believe that lora makes it more tractable to answer this than full there more principled ways to do it finally the of that wcould be as well which can also be a source of inspiration for future works references armen aghajanyan luke zettlemoyer and sonal gupta intrinsic dimensionality explains the effectiveness of language model cs december url http zeyuan and yuanzhi li what can resnet learn efﬁciently going beyond kernels in neurips full version available at http zeyuan and yuanzhi li backward feature correction how deep learning performs deep learning arxiv preprint zeyuan and yuanzhi li feature puriﬁcation how adversarial training performs robust deep learning arxiv preprint zeyuan yuanzhi li and zhao song a convergence theory for deep learning via parameterization in icml full version available at http jimmy lei ba jamie ryan kiros and geoffrey hinton layer normalization tom b brown benjamin mann nick ryder melanie subbiah jared kaplan prafulla wal arvind neelakantan pranav shyam girish sastry amanda askell sandhini agarwal ariel oss gretchen krueger tom henighan rewon child aditya ramesh daniel ziegler jeffrey wu clemens winter christopher hesse mark chen eric sigler mateusz litwin scott gray benjamin chess jack clark christopher berner sam mccandlish alec radford ilya sutskever and dario amodei language models are learners cs july url http cai emmanuel j cand es and zuowei shen a singular value thresholding algorithm for matrix completion siam journal on optimization daniel cer mona diab eneko agirre inigo and lucia specia task semantic textual similarity multilingual and crosslingual focused evaluation proceedings of the international workshop on semantic evaluation doi url http ronan collobert and jason weston a uniﬁed architecture for natural language processing deep neural networks with multitask learning in proceedings of the international conference on machine learning icml pp new york ny usa july association for computing machinery isbn doi url https misha denil babak shakibi laurent dinh marc aurelio ranzato and nando de freitas predicting parameters in deep learning jacob devlin chang kenton lee and kristina toutanova bert of deep bidirectional transformers for language understanding jacob devlin chang kenton lee and kristina toutanova bert of deep bidirectional transformers for language understanding cs may url http arxiv william dolan and chris brockett automatically constructing a corpus of sentential paraphrases inproceedings of the third international workshop on paraphrasing url https claire gardent anastasia shimorina shashi narayan and laura the webnlg challenge generating text from rdf data in proceedings of the international conference on natural language generation pp networks outperform kernel methods arxiv preprint bogdan gliwa iwona mochol maciej biesek and aleksander wawer samsum corpus a annotated dialogue dataset for abstractive summarization corr url http lars grasedyck daniel kressner and christine tobler a literature survey of tensor approximation techniques jihun ham and daniel lee grassmann discriminant analysis a unifying view on learning in icml pp url https karen hambardzumyan hrant khachatrian and jonathan may warp adversarial reprogramming cs december url http arxiv pengcheng he xiaodong liu jianfeng gao and weizhu chen deberta bert with disentangled attention neil houlsby andrei giurgiu stanislaw jastrzebski bruna morrone quentin de laroussilhe andrea gesmundo mona attariyan and sylvain gelly transfer learning for nlp cs stat june url http max jaderberg andrea vedaldi and andrew zisserman speeding up convolutional neural networks with low rank expansions arxiv preprint mikhail khodak neil tenenholtz lester mackey and nicol o fusi initialization and regularization of factorized neural layers diederik kingma and jimmy ba adam a method for stochastic optimization dmitry lepikhin hyoukjoong lee yuanzhong xu dehao chen orhan firat yanping huang maxim krikun noam shazeer and zhifeng chen gshard scaling giant models with conditional computation and automatic sharding brian lester rami and noah constant the power of scale for prompt tuning cs april url http arxiv chunyuan li heerad farkhoor rosanne liu and jason yosinski measuring the intrinsic mension of objective landscapes cs stat april url http arxiv xiang lisa li and percy liang optimizing continuous prompts for generation cs january url http yuanzhi li and yingyu liang learning overparameterized neural networks via stochastic gradient descent on structured data in advances in neural information processing systems yuanzhi li yingyu liang and andrej risteski recovery guarantee of weighted proximation via alternating minimization in international conference on machine learning pp pmlr yuanzhi li tengyu ma and hongyang zhang algorithmic regularization in matrix sensing and neural networks with quadratic activations in conference on learning ory pp pmlr zhaojiang lin andrea madotto and pascale fung exploring versatile generative language model via transfer learning in findings of the association for computational guistics emnlp pp online november association for computational linguistics doi url https understands too cs march url http arxiv yinhan liu myle ott naman goyal jingfei du mandar joshi danqi chen omer levy mike lewis luke zettlemoyer and veselin stoyanov roberta a robustly optimized bert pretraining approach ilya loshchilov and frank hutter decoupled weight decay regularization arxiv preprint ilya loshchilov and frank hutter decoupled weight decay regularization rabeeh karimi mahabadi james henderson and sebastian ruder compacter efﬁcient hypercomplex adapter layers linyong nan dragomir radev rui zhang amrit rau abhinand sivaprasad chiachun hsieh xiangru tang aadit vyas neha verma pranav krishna et al dart structured data record to text generation arxiv preprint jekaterina novikova ond ˇrej du ˇsek and verena rieser the dataset new challenges for generation arxiv preprint samet oymak zalan fabian mingchen li and mahdi soltanolkotabi generalization tees for neural networks via harnessing the structure of the jacobian arxiv preprint jonas pfeiffer aishwarya kamath andreas r kyunghyun cho and iryna gurevych fusion task composition for transfer learning daniel povey gaofeng cheng yiming wang ke li hainan xu mahsa yarmohammadi and jeev khudanpur matrix factorization for deep neural networks in interspeech pp alec radford karthik narasimhan tim salimans and ilya sutskever improving language standing by generative pp alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever language models are unsupervised multitask learners pp pranav rajpurkar robin jia and percy liang know what you don t know unanswerable questions for squad corr url http rebufﬁ hakan bilen and andrea vedaldi learning multiple visual domains with residual adapters cs stat november url http arxiv andreas r gregor geigle max glockner tilman beck jonas pfeiffer nils reimers and iryna gurevych adapterdrop on the efﬁciency of adapters in transformers tara n sainath brian kingsbury vikas sindhwani ebru arisoy and bhuvana ramabhadran rank matrix factorization for deep neural network training with output targets ieee international conference on acoustics speech and signal processing pp ieee mohammad shoeybi mostofa patwary raul puri patrick legresley jared casper and bryan catanzaro training parameter language models using model allelism richard socher alex perelygin jean wu jason chuang christopher manning andrew ng and christopher potts recursive deep models for semantic compositionality over a sentiment treebank in proceedings of the conference on empirical methods in natural language processing pp seattle washington usa october association for tional linguistics url https łukasz kaiser and illia polosukhin attention is all you need in proceedings of the ternational conference on neural information processing systems pp alex wang amanpreet singh julian michael felix hill omer levy and samuel bowman glue a benchmark and analysis platform for natural language understanding alex wang yada pruksachatkun nikita nangia amanpreet singh julian michael felix hill omer levy and samuel bowman superglue a stickier benchmark for language understanding systems alex warstadt amanpreet singh and samuel r bowman neural network acceptability judgments arxiv preprint adina williams nikita nangia and samuel bowman a challenge corpus for tence understanding through inference in proceedings of the conference of the north american chapter of the association for computational linguistics human language gies volume long papers pp new orleans louisiana june association for computational linguistics doi url https thomas wolf lysandre debut victor sanh julien chaumond clement delangue anthony moi pierric cistac tim rault r louf morgan funtowicz joe davison sam shleifer patrick von platen clara ma yacine jernite julien plu canwen xu teven le scao sylvain ger mariama drame quentin lhoest and alexander rush transformers natural language processing in proceedings of the conference on empirical methods in natural language processing system demonstrations pp online october sociation for computational linguistics url https greg yang and edward hu feature learning in neural networks may url http arxiv elad ben zaken shauli ravfogel and yoav goldberg bitﬁt simple for masked yu zhang ekapol chuangsuwanich and james glass extracting deep neural network bottleneck features using matrix factorization in ieee international conference on acoustics speech and signal processing icassp pp ieee yong zhao jinyu li and yifan gong plus diagonal adaptation for deep neural networks ieee international conference on acoustics speech and signal processing icassp pp ieee victor zhong caiming xiong and richard socher generating structured queries from natural language using reinforcement learning corr url http a l arge language models still need parameter updates learning or prompt engineering is very advantageous when we only have a handful of training samples however in practice we can often afford to curate a few thousand or more training examples for applications as shown in table improves the model performance drastically compared to learning on datasets large and small we take the result on rte from the paper brown et for we use two demonstrations per class and six examples in total table signiﬁcantly outperforms learning on brown et b i nference latency introduced by adapter layers adapter layers are external modules added to a model in a sequential manner whereas our proposal lora can be seen as external modules added in a parallel manner consequently adapter layers must be computed in addition to the base model inevitably introducing additional latency while as pointed out in r et al the latency introduced by adapter layers can be mitigated when the model batch size sequence length is large enough to full utilize the hardware parallelism we conﬁrm their observation with a similar latency study on medium and point out that there are scenarios notably online inference where the batch size is small where the added latency can be signiﬁcant we measure the latency of a single forward pass on an nvidia quadro by averaging over trials we vary the input batch size sequence length and the adapter bottleneck dimension we test two adapter designs the original one by houlsby et al which we call adapterh and a recent more efﬁcient variant by lin et al which we call adapterl see section for more details on the designs we plot the in percentage compared to the baseline in figure rseq len seq len seq len batch r batch batch size figure percentage of inference latency compared to the baseline the top row shows the result for adapterhand the bottom row adapterl larger batch size and sequence length help to mitigate the latency but the can be as high as over in an online scenario we tweak the colormap for better visibility c d ataset details glue benchmark is a collection of natural language understanding tasks it includes mnli inference williams et al sentiment analysis socher et al mrpc paraphrase detection dolan brockett cola linguistic acceptability warstadt et al qnli inference rajpurkar et al rte inference standard metric to evaluate nlu models such as roberta and deberta the individual datasets are released under different permissive licenses wikisql is introduced in zhong et al and contains amples the task is to generate sql queries from natural language questions and table schemata we encode context as schema querygand target as the dataset is release under the bsd license samsum is introduced in gliwa et al and contains examples it consists of staged chat conversations between two people and corresponding abstractive summaries written by linguists we encode context as nn concatenated utterances followed by a nnnn and target as the dataset is released under the licence creative commons nlg challenge was ﬁrst introduced in novikova et al as a dataset for training end natural language generation systems and is commonly used for tion the dataset consists of roughly and ples from the restaurant domain each source table used as input can have multiple references each sample input x y consists of a sequence of pairs along with a corresponding natural language reference text the dataset is released under creative commons dart is an dataset described in nan et al dart inputs are structured as sequences of entity relation entity triples with in total dart is a signiﬁcantly larger and more complex task compared to the dataset is released under the mit license webnlg is another commonly used dataset for evaluation gardent et with in total webnlg comprises distinct categories nine of which are seen during training since ﬁve of the total categories are not seen during training but are represented in the test set evaluation is typically broken out by seen categories s unseen categories u and all a each input example is represented by a sequence of subject property object triples the dataset is released under creative commons d h yperparameters used in experiments r obert a we train using adamw with a linear learning rate decay schedule we sweep learning rate number of training epochs and batch size for lora following liu et al we initialize the lora modules to our best mnli checkpoint when adapting to mrpc rte and instead of the usual initialization the model stays frozen for all tasks we report the median over random seeds the result for each run is taken from the best epoch for a fair comparison with the setup in houlsby et al and pfeiffer et al we restrict the model sequence length to and used a ﬁxed batch size for all tasks importantly we start with the roberta large model when adapting to mrpc rte and instead of a model already adapted to mnli the runs with this restricted setup are marked with y see the hyperparameters used in our runs in table d ebert a we again train using adamw with a linear learning rate decay schedule following he et al we tune learning rate dropout probability steps and batch size we use the same model sequence length used by he et to keep our comparison fair following he et al we initialize the lora modules to our best mnli checkpoint when adapting to mrpc rte and instead of the usual initialization the model stays frozen for all tasks we report the median over random seeds the result for each run is taken from the best epoch see the hyperparameters used in our runs in table optimizer adamw warmup ratio lr schedule linear roberta base lorabatch size epochs learning rate lora conﬁg lora max seq len roberta large lorabatch size epochs learning rate lora conﬁg lora max seq len roberta large loraybatch size epochs learning rate lora conﬁg lora max seq len roberta large adptp ybatch size epochs learning rate bottleneckr max seq len roberta large adptp ybatch size epochs learning rate bottleneckr max seq len roberta large adpth ybatch size epochs learning rate bottleneckr max seq len roberta large adpth ybatch size epochs learning rate bottleneckr max seq len table the hyperparameters we used for roberta on the glue benchmark we train all of our models using adamw loshchilov hutter with a linear learning rate schedule for epochs we use the batch size learning rate and beam search beam size described in li liang accordingly we also tune the above hyperparameters for lora we report the mean over random seeds the result for each run is taken from the best epoch the hyperparameters used for lora in are listed in table for those used for other baselines see li liang for all experiments we train using adamw loshchilov hutter for epochs with a batch size of samples and a weight decay factor of we use a sequence length of for optimizer adamw warmup ratio lr schedule linear deberta xxl lorabatch size epochs learning rate weight decay cls dropout lora conﬁg lora max seq len table the hyperparameters for deberta xxl on tasks included in the glue benchmark dataset webnlg dart training optimizer adamw weight decay dropout prob batch size epoch warmup steps learning rate schedule linear label smooth learning rate adaptation lora inference beam size length penalty no repeat ngram size table the hyperparameters for lora on webnlg and dart wikisql zhong et for mnli williams et and for samsum gliwa et we tune learning rate for all combinations see section for more details on the hyperparameters used for tuning we ﬁnd the optimal lpandli to be and respectively totalling parameters we use for tuning with parameters to obtain the overall best performance we present two parameter budgets for lora and we report the best validation performance from each run the training hyperparameters used in our experiments are listed in table e c ombining lora with prefix tuning lora can be naturally combined with existing approaches in this section we evaluate two combinations of lora and variants of on wikisql and mnli combines lora with tuning where we insert tokens whose embeddings are treated as trainable parameters for more on embedding tuning see section combines lora with tuning we also insert special tokens however instead of letting the hidden representations of these tokens evolve optimizer adamw batch size epoch warmup tokens lr schedule linear learning rate table the training hyperparameters used for different adaption methods we use the same hyperparameters for all datasets after tuning learning rate rally we replace them after every transformer block with an input agnostic vector thus both the embeddings and subsequent transformer block activations are treated as trainable parameters for more on tuning see section in table we show the evaluation results of and on wikisql and multinli first of all signiﬁcantly outperforms both lora and tuning on wikisql which indicates that lora is somewhat orthogonal to tuning on multinli the combination of doesn t perform better than lora possibly because lora on its own already achieves performance comparable to the human baseline secondly we notice that performs slightly worse than lora even with more trainable parameters we tribute this to the fact that tuning is very sensitive to the choice of learning rate and thus makes the optimization of lora weights more difﬁcult in f a dditional empirical experiments a dditional experiments on we also repeat our experiment on dart nan et and webnlg gardent et following the setup of li liang the result is shown in table similar to our result on nlg challenge reported in section lora performs better than or at least with approaches given the same number of trainable parameters method trainable dart parameters bleu met ter medium preflayer lora large preflayer lora table with different adaptation methods on dart the variances of met and ter are less than all adaption approaches bleu met ter u s a u s a u s a medium adapterl adapterl preﬁx lora large adapterl adapterl preﬁx lora table with different adaptation methods on webnlg the variances of met and ter are less than all the experiments we ran u indicates unseen categories s indicates seen categories and a indicates all categories in the test set of webnlg a dditional experiments on we present additional runs on with different adaptation methods in table the focus is on identifying the between performance and the number of trainable parameters l regime to evaluate the performance of different adaptation approaches in the regime we randomly sample and training examples from the full training set of mnli to form the in table we show the performance of different adaptation approaches on to our surprise preﬁxembed and preﬁxlayer performs very poorly on dataset with preﬁxembed performing only slightly better than random chance preﬁxlayer performs better than preﬁxembed but is still signiﬁcantly worse than or lora on the gap between approaches and becomes smaller as we crease the number of training examples which might suggest that approaches are not suitable for tasks in lora achieves better performance than on both and and comparable results on and considering the variance due to random seeds the training hyperparameters of different adaptation approaches on are reported in ble we use a smaller learning rate for preﬁxlayer on the set as the training loss does not decrease with a larger learning rate g m easuring similarity between subspaces in this paper we use the measure a b i j ui a uj b f minfi jgto measure the subspace similarity between two column orthonormal matrices ui obtained by taking columns of the left singular matrices of aandb we point out that this similarity is simply a reverse of the standard projection metric that measures distance between subspaces ham lee m m m m m m m m m m m m m m m m m m m m m m m m m m m m m table hyperparameter analysis of different adaptation approaches on wikisql and mnli both tuning preﬁxembed and tuning preﬁxlayer perform worse as we increase the number of trainable parameters while lora s performance stabilizes performance is measured in validation accuracy method mnli m mnli m mnli m mnli m preﬁxembed preﬁxlayer lora table validation accuracy of different methods on subsets of mnli using ndescribes a subset with ntraining examples we evaluate with the full validation set lora performs exhibits favorable compared to other methods including to be concrete let the singular values of ui auj bto minfi jg we know that the projection metric ham lee is deﬁned as d ui a uj b pp optimizer adamw warmup tokens lr schedule linear batch size epoch learning ratefinetune preﬁxembed preﬁxlayer lora preﬁxembed lp preﬁxembed li speciﬁc preﬁxtune lora table the hyperparameters used for different adaptation methods on mnli m where our similarity is deﬁned as a b i j ui a uj b i ui a uj b this similarity satisﬁes that if ui aanduj bshare the same column span then a b i j if they are completely orthogonal then a b i j otherwise a b i j h a dditional experiments on matrices we present additional results from our investigation into the update matrices c orrelation between lora m odules see figure and figure for how the results presented in figure and figure generalize to other layers e ffect we repeat our experiment on the effect of r section in using the nlg challenge dataset as an example we report the validation loss and test metrics achieved by different choices ofrafter training for steps we present our result in table the optimal rank for medium is between and depending on the metric used which is similar to that for note that the relationship between model size and the optimal rank for adaptation is still an open question c orrelation between wand see figure for the normalized subspace similarity between varying note again that not contain the top singular directions of w since the similarity between the top directions in the of those in wbarely exceeds this gives evidence those directions that are otherwise notemphasized in an interesting next question to answer is how strong do we need to amplify those directions in order for the model adaptation to work well iwq wv wq wv i i i j i j figure normalized subspace similarity between the column vectors of both the and layers in a transformer a mplification factor one can naturally consider a feature ampliﬁcation factor as the ku wv kf whereuandv are the and matrices of the svd decomposition of recalluu wv v gives the projection of wonto the subspace spanned by intuitively when contains directions this quantity measures how much of them are ampliﬁed by as shown in section for this ampliﬁcation factor is as large as in other words there are generally speaking four feature directions in each layer out of the entire feature space from the model w that need to be ampliﬁed by a very large factor in order to achieve our reported accuracy for the downstream speciﬁc task and one should expect a very different set of feature directions to be ampliﬁed for each different downstream task one may notice however for this ampliﬁcation factor is only around meaning that most directions learned in arenotbeing ampliﬁed by much this should not be surprising and in fact gives evidence once again that the intrinsic rank needed to represent the directions thus for model adaptation is low in contrast those directions in the version of corresponding to are ampliﬁed by a much larger factor iwq wv layer wv i j jlayer j i j figure normalized subspace similarity between the column vectors of two randomly seeded runs for both the and layers in a former rankrvalloss bleu nist meteor rouge l cider table validation loss and test set metrics on nlg challenge achieved by lora with different rank rusing medium unlike on where for many tasks here the performance peaks at for validation loss and for bleu suggesting the medium has a similar intrinsic rank for adaptation compared to note that some of our hyperparameters are tuned on which matches the parameter count of another baseline and thus might not be optimal for other choices of wq i j jwq wq i j j wq i j jrandom wq arand i j figure normalized subspace similarity between the singular directions of wqand those of with varying rand a random baseline directions that are important but not sized a larger rtends to pick up more directions that are already emphasized in"
}